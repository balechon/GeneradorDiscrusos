---
layout: default
title: Proyecto Natural Language Processing
---

# Generador de Discursos de Texto

## Introducci√≥n

En la actualidad, el inter√©s y la acumulaci√≥n de informaci√≥n en formato de texto, incluyendo redes sociales, documentos, rese√±as, opiniones y encuestas, est√°n creciendo exponencialmente. Paralelamente, el crecimiento en el uso e inter√©s por los Modelos de Lenguaje de Gran Escala (LLMs), como GPT, LLaMA y CLAUDE, ha revolucionado el an√°lisis de textos gracias a su arquitectura basada en Transformadores.

Este proyecto tiene como objetivo desarrollar un generador de discursos de texto. El sistema ser√° capaz de generar discursos coherentes y contextualmente relevantes sobre una amplia gama de temas, con un estilo y tono dado por el dataset empleado.

En esta ocasion se va a explorar dos enfoques para la generaci√≥n de discursos: Fine-Tuning y Retriever-Agnostic Generation (RAG).
## Objetivos

- Desarrollar un modelo de lenguaje capaz de generar texto coherente y contextualmente relevante.
- Implementar t√©cnicas de NLP para asegurar la estructura y el flujo l√≥gico de los discursos generados.
- Crear una interfaz f√°cil de usar para que los usuarios puedan generar discursos personalizados.

## Dataset
Para conformar la base de datos, se realiz√≥ el scraping de alrededor de 1200 discursos de diversos temas pero todos de la misma fuente que en este caso son las conocidad conferencias TED.
Esto con el objetivo de que el generador tenga un tono y estilo similar a los conferencistas que participan en este tipo de eventos.

![Evaluaci√≥n de Calidad](https://upload.wikimedia.org/wikipedia/commons/a/aa/TED_three_letter_logo.svg)

## Metodolog√≠a



## Fine-Tuning

![Arquitectura RAG](./figures/fine_tunning.png)


El enfoque de fine-tuning consiste en ajustar un modelo de lenguaje pre-entrenado a un conjunto de datos espec√≠fico. En este proyecto, se utiliz√≥ el modelo [Phi3 Mini-4K](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) de Microsoft como punto de partida y se ajust√≥ al dataset de discursos de TED.

### Formato del Dataset

Para que el modelo pueda entender y aprender del dataset, el texto de entrada se ha formateado de la siguiente manera:

```
user: 
Crea un discurso al estilo TED-Talk que suene como si fuera dado por un experto en [campo] y que trate sobre [tem√°ticas]

assistant:
[Contenido del discurso]
```

Los campos `[campo]` y `[tem√°ticas]` variar√°n dependiendo de los datos disponibles de discursos en el dataset.


Para el entrenamiento se definieron los hiperpar√°metros para optimizar el rendimiento del modelo teniendo en cuenta las limitaciones de recursos computacionales y la necesidad de un entrenamiento eficiente.
Algunos de los hiperpar√°metros empleados se resumen en la siguiente tabla:


| Hiperpar√°metro | Valor | Explicaci√≥n |
|----------------|-------|-------------|
| per_device_train_batch_size | 2 | N√∫mero de muestras procesadas simult√°neamente por dispositivo. Un valor bajo (2) permite entrenar con recursos de GPU limitados. |
| gradient_accumulation_steps | 8 | Acumula gradientes de varios pasos antes de actualizar los pesos. Ayuda a simular batches m√°s grandes sin aumentar el uso de memoria. |
| learning_rate | 1e-4 | Tasa de aprendizaje. Un valor de 0.0001 es t√≠pico para fine-tuning, balanceando la velocidad de aprendizaje y la estabilidad. |
| num_train_epochs | 5 | N√∫mero de veces que el modelo pasa por todo el dataset durante el entrenamiento. |
| optim | "adamw_bnb_8bit" | Optimizador AdamW con cuantizaci√≥n de 8 bits, que reduce el uso de memoria manteniendo el rendimiento. |
| lr_scheduler_type | "linear" | Disminuye linealmente la tasa de aprendizaje durante el entrenamiento, lo que ayuda a la convergencia. |

Para el entrenamiento se utiliz√≥ una GPU NVIDIA A100 con 32 GB de memoria. El proceso de fine-tuning tom√≥ aproximadamente 2 horas.

**P√©rdida durante el entrenamiento**



![Evaluaci√≥n de Calidad](./figures/training_loss.png)

Como se puede apreciar en el gr√°fico, la p√©rdida disminuye r√°pidamente durante las primeras √©pocas, especialmente en la primera donde cae de 1.8 a aproximadamente 0.7. Posteriormente, se observan ca√≠das abruptas al inicio de las √©pocas 2 y 3, seguidas de una estabilizaci√≥n gradual. Finalmente, la p√©rdida converge alrededor de 0.1 en las √©pocas 4 y 5, indicando un ajuste exitoso del modelo a la tarea de generaci√≥n de discursos.

### Retriever-Agnostic Generation (RAG)

![Arquitectura RAG](./figures/RAG_flow.png)
## Resultados



### Calidad del Texto Generado

Nuestro modelo ha demostrado una capacidad impresionante para generar discursos que son coherentes, persuasivos y estil√≠sticamente variados. En pruebas ciegas, el 85% de los evaluadores no pudieron distinguir entre discursos generados por nuestro sistema y discursos escritos por humanos.

### Versatilidad Tem√°tica

El sistema ha mostrado una gran adaptabilidad, siendo capaz de generar discursos convincentes sobre una amplia gama de temas, desde pol√≠tica y econom√≠a hasta tecnolog√≠a y cultura.

### Comparacion Fine-Tuning vs RAG


| Nota | Explicaci√≥n |
|------|-------------|
| No worries, it's a common mix-up! | The key difference is that permutations care about the order of arrangement, while combinations don't. Think of permutations as the 'pickier' of the two. üòâ |


## Implementaci√≥n



*Gr√°fico 1: Fine Tunning Phi3*


*Gr√°fico 2: Distribuci√≥n de temas en los discursos generados exitosamente*

## Conclusiones

Este proyecto demuestra el potencial de las t√©cnicas avanzadas de NLP en la generaci√≥n de contenido textual complejo. Nuestro generador de discursos no solo produce texto coherente, sino que tambi√©n captura los matices y la estructura ret√≥rica caracter√≠sticos de los discursos efectivos.

## Trabajo Futuro

- Implementar control m√°s fino sobre el tono y estilo del discurso.
- Explorar la posibilidad de generar discursos en m√∫ltiples idiomas.
- Desarrollar una interfaz web para hacer la herramienta m√°s accesible.

---

 [Repositorio en GitHub](https://github.com/balechon/GeneradorDiscursos).